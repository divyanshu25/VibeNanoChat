# ğŸš€ NanoGPT

> A clean, educational implementation of GPT-2 (124M parameters) trained on the OpenWebText dataset. Learn how transformers work by building one yourself!

## âœ¨ What's Inside

- **ğŸ§  GPT-2 Architecture** (124M parameters): 12 layers, 12 attention heads, 768 dimensions of pure transformer magic
- **âš¡ Distributed Training**: Scale across multiple GPUs with PyTorch DDP
- **ğŸ“š OpenWebText Dataset**: Train on ~9 billion tokens scraped from the web
- **ğŸ¯ Modern Training Stack**: Mixed precision (bfloat16), gradient clipping, cosine LR scheduling
- **ğŸ“Š Experiment Tracking**: Built-in Weights & Biases integration
- **ğŸ’¾ Efficient Data Loading**: Memory-mapped binary files for lightning-fast I/O

## ğŸ¬ Getting Started

### 1. Clone & Setup

```bash
git clone https://github.com/yourusername/NanoGPT.git
cd NanoGPT
```

### 2. ğŸ› ï¸ Install Dependencies

We use [UV](https://github.com/astral-sh/uv) because it's blazingly fast:

```bash
# ğŸ¯ One command to rule them all
make environment

# Or step by step if you're old school:
make uv        # Install UV
make uvlock    # Lock dependencies
make venv      # Create virtual environment
```

### 3. ğŸ“Š Prepare the OpenWebText Dataset

Time to download and tokenize ~9 billion tokens of internet wisdom:

```bash
cd src/data/openwebtext
uv run python prepare.py
```

**What happens next:**
- ğŸ“¥ Downloads OpenWebText dataset from HuggingFace (~54GB raw)
- ğŸ”¤ Tokenizes everything with GPT-2 BPE encoding
- ğŸ’¾ Saves to `/sensei-fs/users/divgoyal/openwebtext/` (update path as needed)
- âœ… Creates: `train.bin` (~17GB, 9B tokens) and `val.bin` (~8.5MB, 4M tokens)

â˜• Grab some coffee - this takes ~15-30 minutes depending on your connection!

### 4. ğŸ”¥ Train the Model

#### Single GPU Training

```bash
python src/gpt_2/ddp.py
```

#### ğŸš„ Multi-GPU Training (Go Fast!)

```bash
# Train with 8 GPUs (recommended for speed)
make ddp-train NGPUS=8

# Got 4 GPUs? No problem!
make ddp-train NGPUS=4

# Or go manual with torchrun:
torchrun --standalone --nproc_per_node=8 src/gpt_2/ddp.py
```

**âš™ï¸ Training Configuration (The Sweet Spot):**
- ğŸ“¦ Batch size per GPU: 64
- ğŸ“ Sequence length: 1024 tokens
- ğŸ¯ Total batch size: 524,288 tokens/step (2^19, perfectly balanced as all things should be)
- ğŸ“ Max learning rate: 6e-4 (with 715 warmup steps)
- ğŸƒ Total steps: 17,234 (one full epoch over 9B tokens)
- ğŸ’ª Optimizer: AdamW with weight decay 0.1
- âœ‚ï¸ Gradient clipping: 1.0 (keeps those gradients in check)

## ğŸ›ï¸ Configuration Deep Dive

### Model Config (GPT-2 124M)

```python
block_size: 1024      # Context window size
vocab_size: 50257     # GPT-2 vocabulary (BPE)
n_layer: 12           # Transformer blocks (the secret sauce)
n_head: 12            # Attention heads (parallel thoughts)
n_embed: 768          # Embedding dimension (the hidden state)
```

### Training Config

```python
max_learning_rate: 6e-4          # Peak LR (after warmup)
min_learning_rate: 6e-5          # Final LR (10% of max)
warmup_steps: 715                # Linear warmup phase
total_batch_size: 524288         # Tokens per optimization step
weight_decay: 0.10               # L2 regularization
gradient_clip_norm: 1.0          # Gradient explosion prevention
```

## ğŸ“ˆ Monitoring Your Training

Training metrics auto-log to **Weights & Biases**:
- ğŸ“‰ Training loss (watch it go down!)
- ğŸ“Š Learning rate schedule (that beautiful cosine decay)
- âš¡ Tokens per second (throughput metrics)
- ğŸ“ Gradient norms (stability indicators)

ğŸ‘‰ View your runs at: https://wandb.ai/

## ğŸ› ï¸ Handy Commands

```bash
# ğŸ“Š Check GPU status
make gpu-status

# ğŸ”ª Kill all GPU processes (nuclear option)
make kill-gpu

# ğŸ”¥ Keep GPUs warm for testing
make gpu-hot GPUS=0,1,2
```

## ğŸ“š Dataset Details

**OpenWebText: The Internet in a Box**
- ğŸ”— Source: [Skylion007/openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext)
- ğŸ“¦ Size: ~8 million documents, ~9 billion tokens
- ğŸ”¤ Processing: GPT-2 BPE tokenization with end-of-text markers
- ğŸ’¾ Storage: Efficient binary format (uint16) for blazing-fast loading

## âš¡ Performance Benchmarks

**Expected Throughput** (your mileage may vary):

| Hardware | Tokens/Second | Time per Epoch |
|----------|---------------|----------------|
| 8x A100 80GB | ~350K | ~7 hours â° |
| 8x H100 80GB | ~600K | ~4 hours ğŸš€ |

*Training 9 billion tokens has never been this fast!*

## ğŸ’¡ Pro Tips

1. **ğŸ® Memory Management**: With batch_size=64 and block_size=1024, budget ~40GB VRAM per GPU
2. **ğŸ”„ Gradient Accumulation**: Auto-calculated based on GPU count and target batch size (we do the math for you!)
3. **ğŸ’¾ Checkpointing**: Models saved periodically during training (no progress lost!)
4. **âš¡ Mixed Precision**: Uses bfloat16 for 2x speedup and 50% memory savings

## ğŸ”§ Troubleshooting

**ğŸ˜± Out of Memory Error?**
- Turn down `batch_size` in `gpt2_model.py`
- The system auto-adjusts gradient accumulation steps (smart!)

**ğŸŒ Data Loading Slow?**
- Network filesystems don't support mmap (it's okay, we have a fallback)
- Pro tip: Copy data to local SSD for maximum zoom

**ğŸ¤” Distributed Training Not Working?**
- Check NCCL installation: `python -c "import torch; print(torch.cuda.nccl.version())"`
- Verify GPUs visible: `nvidia-smi`
- Make sure all GPUs are the same model (mixed GPU types = sadness)

## ğŸ“ Learning Resources

Want to understand what's happening under the hood?

- ğŸ“º [Andrej Karpathy's GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- ğŸ“„ [Attention is All You Need](https://arxiv.org/abs/1706.03762) (the paper that started it all)
- ğŸ“š [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

## ğŸ™ Acknowledgements

Standing on the shoulders of giants:

- Inspired by [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT) - the OG educational GPT
- Based on OpenAI's GPT-2 architecture - thank you for open-sourcing!
- Dataset: [OpenWebText](https://huggingface.co/datasets/Skylion007/openwebtext) - internet gold

## ğŸ“œ License

MIT License - Go build something cool!

---

<div align="center">

**Built with â¤ï¸ for learning and experimentation**

If this helped you understand transformers better, â­ star the repo!

</div>
